name: Run scraper
on:
  push:
  schedule:
    # trigger every day at 13 UTC
    - cron:  '0 13 * * *'
env:
  MY_AWS_ACCESS_KEY_ID: ${{ secrets.MY_AWS_ACCESS_KEY_ID }}
  MY_AWS_SECRET_ACCESS_KEY: ${{ secrets.MY_AWS_SECRET_ACCESS_KEY }}
  MY_AWS_DEFAULT_REGION: ${{ secrets.MY_AWS_DEFAULT_REGION }}
  CITY_COUNCIL_SCRAPER_S3_BUCKET: ${{ secrets.CITY_COUNCIL_SCRAPER_S3_BUCKET }}
jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Check out code
        uses: actions/checkout@v2

      - name: Build container
        run: |
          docker build \
          --build-arg MY_AWS_ACCESS_KEY_ID=${MY_AWS_ACCESS_KEY_ID} \
          --build-arg MY_AWS_SECRET_ACCESS_KEY=${MY_AWS_SECRET_ACCESS_KEY} \
          --build-arg MY_AWS_DEFAULT_REGION=${MY_AWS_DEFAULT_REGION} \
          --build-arg CITY_COUNCIL_SCRAPER_S3_BUCKET=${CITY_COUNCIL_SCRAPER_S3_BUCKET} \
          -t etl-and-scrape . \
          -f ci/Dockerfile

      - name: Run scraper
        run: docker run etl-and-scrape
